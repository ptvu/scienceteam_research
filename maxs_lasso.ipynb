{
 "metadata": {
  "name": "",
  "signature": "sha256:911a88b45751f2afc895d754c6b180d44811c1b4fd377afdd3b5157d41766e07"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###################################################################\n",
      "# Creates a term document matrix from movie reviews data          #\n",
      "###################################################################\n",
      "import sys\n",
      "import os\n",
      "import glob\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "START_K = 0\n",
      "K = 400\n",
      "\n",
      "# Movie data corpus\n",
      "negative_files = glob.glob(\"movie_review_data/neg/*.txt\")\n",
      "positive_files = glob.glob(\"movie_review_data/pos/*.txt\")\n",
      "all_file_names = negative_files + positive_files\n",
      "\n",
      "# Load valid english words\n",
      "valid_words = set()\n",
      "dictionary_file = open(\"words\")\n",
      "for word in dictionary_file:\n",
      "    word = word.lower().strip()\n",
      "    valid_words.add(word)\n",
      "    \n",
      "# Load english stop words\n",
      "stop_words = set()\n",
      "stop_words_file = open(\"stop_words\")\n",
      "for word in stop_words_file:\n",
      "    word = word.lower().strip()\n",
      "    stop_words.add(word)\n",
      "\n",
      "# Filter even more. Only take the K top most frequent words from the combined corpus\n",
      "word_frequencies = {}\n",
      "corpus_words = set()\n",
      "for file_name in all_file_names:\n",
      "    f = open(file_name)\n",
      "    text = unicode(f.read(), 'latin-1')\n",
      "    for word in text.split(\" \"):\n",
      "        word = word.lower().strip()\n",
      "        if word in valid_words and word not in stop_words:\n",
      "            if word not in word_frequencies:\n",
      "                word_frequencies[word] = 0\n",
      "            word_frequencies[word] += 1\n",
      "            corpus_words.add(word)\n",
      "    f.close()\n",
      "valid_features = sorted(corpus_words, key=lambda x: word_frequencies[x], reverse=True)[START_K:START_K+K]\n",
      "                \n",
      "# Collect all movie review text (both positive and negatively sentimented)\n",
      "corpus = []\n",
      "for count, file_name in enumerate(all_file_names):\n",
      "    f = open(file_name)\n",
      "    document_string = unicode(f.read(), 'latin-1')\n",
      "    filtered_document_string = \"\"\n",
      "    for word in document_string.split(\" \"):\n",
      "        word = word.lower().strip()\n",
      "        if word in valid_features:\n",
      "            filtered_document_string += word + ' '\n",
      "    corpus.append(filtered_document_string)\n",
      "    f.close()\n",
      "print(\"Done creating corpus!\")\n",
      "\n",
      "# Use Sklearn to get the term document matrix\n",
      "vectorizer = TfidfVectorizer(min_df=1, stop_words=\"english\")\n",
      "term_doc_matrix = vectorizer.fit_transform(corpus)\n",
      "\n",
      "# tdm and features\n",
      "tdm = term_doc_matrix.toarray()\n",
      "features = vectorizer.get_feature_names()\n",
      "\n",
      "print(\"Number of rows: %d cols: %d\" % tdm.shape)\n",
      "print(tdm)\n",
      "print(\"Features:\")\n",
      "print(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done creating corpus!\n",
        "Number of rows: 1400 cols: 380"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.          0.          0.07095089 ...,  0.          0.          0.        ]\n",
        " [ 0.          0.          0.         ...,  0.          0.          0.09211777]\n",
        " [ 0.          0.22519429  0.         ...,  0.          0.          0.        ]\n",
        " ..., \n",
        " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " [ 0.06270112  0.          0.         ...,  0.          0.          0.        ]]\n",
        "Features:\n",
        "[u'able', u'act', u'acting', u'action', u'actor', u'actress', u'actually', u'age', u'ago', u'alien', u'american', u'annoying', u'apparently', u'art', u'aspect', u'attempt', u'attention', u'audience', u'bad', u'based', u'basically', u'battle', u'beautiful', u'begin', u'beginning', u'believe', u'ben', u'bit', u'black', u'blood', u'bob', u'body', u'book', u'boring', u'boy', u'brief', u'bring', u'brother', u'bruce', u'business', u'camera', u'car', u'care', u'career', u'cast', u'chance', u'change', u'character', u'child', u'chris', u'city', u'classic', u'close', u'comedy', u'comes', u'comic', u'coming', u'complete', u'completely', u'computer', u'cool', u'country', u'couple', u'course', u'crew', u'crime', u'dark', u'daughter', u'david', u'day', u'days', u'dead', u'deal', u'death', u'deep', u'despite', u'dialogue', u'die', u'difficult', u'directed', u'direction', u'director', u'dog', u'doing', u'drama', u'dramatic', u'earth', u'easily', u'easy', u'effects', u'emotional', u'enjoy', u'entertaining', u'entertainment', u'entire', u'especially', u'eventually', u'evil', u'exactly', u'example', u'expect', u'experience', u'extremely', u'fall', u'family', u'father', u'feature', u'feel', u'fiction', u'fight', u'film', u'final', u'finally', u'fine', u'flick', u'following', u'forced', u'form', u'free', u'friend', u'fun', u'funny', u'future', u'game', u'genre', u'george', u'getting', u'girl', u'giving', u'goes', u'guess', u'guy', u'half', u'hand', u'happen', u'happy', u'hard', u'harry', u'head', u'heart', u'help', u'hero', u'hilarious', u'history', u'hit', u'hollywood', u'home', u'hope', u'horror', u'hour', u'house', u'huge', u'human', u'humor', u'husband', u'idea', u'instead', u'involved', u'jack', u'james', u'job', u'joe', u'john', u'kevin', u'kids', u'kill', u'killer', u'king', u'lack', u'language', u'late', u'laugh', u'lead', u'leads', u'learn', u'leave', u'lee', u'left', u'level', u'life', u'line', u'little', u'live', u'living', u'local', u'look', u'looking', u'lost', u'lot', u'love', u'main', u'major', u'mark', u'martin', u'material', u'matter', u'max', u'maybe', u'mean', u'meet', u'message', u'michael', u'mind', u'mission', u'moment', u'money', u'mother', u'motion', u'movie', u'murder', u'music', u'mystery', u'nearly', u'nice', u'night', u'note', u'novel', u'obvious', u'obviously', u'office', u'oh', u'original', u'oscar', u'park', u'particularly', u'past', u'paul', u'people', u'perfect', u'performance', u'person', u'personal', u'peter', u'picture', u'piece', u'planet', u'play', u'plot', u'police', u'poor', u'power', u'predictable', u'premise', u'pretty', u'prison', u'private', u'probably', u'produced', u'production', u'question', u'quickly', u'rated', u'rating', u'read', u'real', u'reason', u'recent', u'relationship', u'release', u'remember', u'rest', u'result', u'return', u'review', u'rich', u'richard', u'robert', u'robin', u'role', u'romance', u'romantic', u'run', u'running', u'sam', u'save', u'scene', u'school', u'science', u'score', u'scott', u'scream', u'screen', u'screenplay', u'script', u'secret', u'seeing', u'seen', u'sense', u'sequel', u'sequence', u'series', u'set', u'sex', u'sexual', u'ship', u'short', u'shot', u'similar', u'simple', u'simply', u'single', u'smart', u'smith', u'society', u'somewhat', u'son', u'soon', u'sort', u'sound', u'space', u'special', u'star', u'starring', u'start', u'steve', u'stop', u'story', u'strong', u'stuff', u'stupid', u'style', u'subject', u'success', u'successful', u'summer', u'supporting', u'supposed', u'surprise', u'surprisingly', u'suspense', u'taking', u'tale', u'talent', u'talk', u'talking', u'team', u'television', u'tell', u'theater', u'thriller', u'time', u'times', u'title', u'tom', u'town', u'true', u'truly', u'try', u'trying', u'unfortunately', u'usually', u'van', u'version', u'video', u'violence', u'visit', u'visual', u'voice', u'war', u'watch', u'watching', u'water', u'white', u'wife', u'wild', u'william', u'woman', u'wonder', u'wonderful', u'word', u'world', u'worse', u'worst', u'worth', u'writer', u'writing', u'written', u'wrong', u'yes', u'york']\n"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###################################################################\n",
      "# Searches query term within list of corpus                       #\n",
      "###################################################################\n",
      "\n",
      "query = unicode('actor')\n",
      "query_presence_indicators = []\n",
      "\n",
      "# Create the indicator column\n",
      "if query in features:\n",
      "    location_in_matrix = features.index(query)\n",
      "    features.remove(query)\n",
      "    \n",
      "    # Remove column with query term from tdm\n",
      "    tdm = np.delete(tdm, location_in_matrix, axis=1)\n",
      "    \n",
      "    # Construct indicator vector\n",
      "    for document in tdm:\n",
      "        if document[location_in_matrix] != 0:\n",
      "            query_presence_indicators.append(1)\n",
      "        else:\n",
      "            query_presence_indicators.append(-1)\n",
      "else:\n",
      "    print(\"Query not found\")\n",
      "\n",
      "print(\"Number of documents\", len(query_presence_indicators))\n",
      "print(\"Number of times query appears in all documents: \", sum([1 if x == 1 else 0 for x in query_presence_indicators]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Number of documents', 1400)\n",
        "('Number of times query appears in all documents: ', 116)\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cvxpy import *\n",
      "import numpy as np\n",
      "import cvxopt\n",
      "from multiprocessing import Pool\n",
      "\n",
      "# Problem data.\n",
      "A = tdm\n",
      "b = query_presence_indicators\n",
      "gamma = Parameter(sign=\"positive\")\n",
      "\n",
      "# Construct the problem.\n",
      "x = Variable(len(A[0]))\n",
      "objective = Minimize(sum_squares(A*x - b) + gamma*norm(x, 1))\n",
      "p = Problem(objective)\n",
      "\n",
      "# Assign a value to gamma and find the optimal x.\n",
      "def get_x(gamma_value):\n",
      "    gamma.value = gamma_value\n",
      "    result = p.solve()\n",
      "    return x.value\n",
      "\n",
      "gammas = np.logspace(-1, 2, num=100)\n",
      "\n",
      "# Parallel computation.\n",
      "print(\"Starting computation\")\n",
      "pool = Pool(processes=1)\n",
      "lasso_weights = pool.map(get_x, gammas)\n",
      "\n",
      "# Serial computation.\n",
      "lasso_weights = [get_x(value) for value in gammas]\n",
      "\n",
      "for v1,v2 in zip(x_values, lasso_weights):\n",
      "    if np.linalg.norm(v1 - v2) > 1e-5:\n",
      "        print \"error\"\n",
      "        \n",
      "print(\"Done!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting computation\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done!\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Copyright (c) 2014 Steve Yadlowsky, Preetum Nakkarin.\n",
      "# Licensed under MIT License.\n",
      "# More information including the exact terms of the License\n",
      "# can be found in the file COPYING in the project root directory.\n",
      "\n",
      "import numpy as np\n",
      "import time\n",
      "import operator\n",
      "import scipy.sparse\n",
      "\n",
      "class IHTClassifier(object):\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.training_time = 0.0\n",
      "        self.beta = None\n",
      "\n",
      "    def card(self, x):\n",
      "        return np.sum(x != 0)\n",
      "\n",
      "    def train(self, X, y, card=50, verbose=False):\n",
      "        start = time.time()\n",
      "        if verbose:\n",
      "            print \"Preconditioning matrix\"\n",
      "        whitened_X, feature_avg = self.whiten_features(X)\n",
      "        lsv = float(self.compute_lsv(whitened_X, feature_avg))\n",
      "        if verbose:\n",
      "            print \"Matching pursuits\"\n",
      "        x_hat = self.matching_pursuit_sparse(y, whitened_X/lsv, feature_avg/lsv, card)\n",
      "        if verbose:\n",
      "            print \"Running iterative hard thresholding\"\n",
      "        self.beta = self.AIHT_sparse(y, whitened_X/lsv, x_hat, card, feature_avg/lsv)/lsv\n",
      "\n",
      "        self.training_time += time.time() - start\n",
      "\n",
      "    def whiten_features(self, X):\n",
      "        X = X.tocsr(copy=True)\n",
      "        row_avg = np.bincount(X.indices, weights=X.data)\n",
      "        row_avg /= float(X.shape[0])\n",
      "        row_norm = np.bincount(X.indices, weights=(X.data - row_avg[X.indices])**2)\n",
      "        nonzeros_in_each_column = np.diff(X.tocsc().indptr)\n",
      "        avg_norm = ((float(X.shape[0])*np.ones(X.shape[1])) - nonzeros_in_each_column)*(row_avg**2)\n",
      "        row_norm += avg_norm\n",
      "        row_norm = np.array([np.sqrt(x) if x != 0 else 1 for x in row_norm])\n",
      "        row_avg /= row_norm\n",
      "        X.data /= np.take(row_norm, X.indices)\n",
      "        feature_avg = np.squeeze(row_avg)\n",
      "\n",
      "        return X, feature_avg\n",
      "\n",
      "    def compute_lsv(self, X, feature_avg):\n",
      "        def matmuldyad(v):\n",
      "            return X.dot(v) - feature_avg.dot(v)\n",
      "\n",
      "        def rmatmuldyad(v):\n",
      "            return X.T.dot(v) - v.sum()*feature_avg\n",
      "        normalized_lin_op = scipy.sparse.linalg.LinearOperator(X.shape, matmuldyad, rmatmuldyad)\n",
      "\n",
      "        def matvec_XH_X(v):\n",
      "            return normalized_lin_op.rmatvec(normalized_lin_op.matvec(v))\n",
      "\n",
      "        which='LM'\n",
      "        v0=None\n",
      "        maxiter=None\n",
      "        return_singular_vectors=False\n",
      "\n",
      "        XH_X = scipy.sparse.linalg.LinearOperator(matvec=matvec_XH_X, dtype=X.dtype, shape=(X.shape[1], X.shape[1]))\n",
      "        eigvals = scipy.sparse.linalg.eigs(XH_X, k=1, tol=0, maxiter=None, ncv=10, which=which, v0=v0, return_eigenvectors=False)\n",
      "        lsv = np.sqrt(eigvals)\n",
      "        return lsv[0].real\n",
      "\n",
      "    def matching_pursuit_sparse(self, y, X, feature_avg, k, tol=10**-10):\n",
      "        '''\n",
      "        Matching Pursuit\n",
      "        '''\n",
      "        r = y\n",
      "        X = X.tocsc()\n",
      "        err_norm = np.linalg.norm(r, 2)\n",
      "        err_norm_prev = 0\n",
      "        beta = np.zeros(X.shape[1])\n",
      "        while self.card(beta) < k:\n",
      "            all_inner_products = X.T.dot(r) - np.sum(r)*feature_avg\n",
      "            max_index, max_abs_inner_product = max(enumerate(np.abs(all_inner_products)), key=operator.itemgetter(1))\n",
      "            g = X[:, max_index]\n",
      "            g = np.squeeze(np.asarray(g.todense())) - feature_avg[max_index]\n",
      "            a = all_inner_products[max_index]\n",
      "            a /= np.linalg.norm(g, 2)**2\n",
      "            beta[max_index] += a\n",
      "            r = r - a*g\n",
      "            err_norm_prev = err_norm\n",
      "            err_norm = np.linalg.norm(r, 2)\n",
      "            if np.abs(err_norm - err_norm_prev) <= tol:\n",
      "                break\n",
      "        return beta\n",
      "\n",
      "    def thresholder(self, y,m):\n",
      "        sort_y = sorted(np.abs(y))\n",
      "        thresh = sort_y[-m]\n",
      "\n",
      "        non_thresholded_indices = (np.abs(y) > thresh)\n",
      "        n_nonzero_indices = sum(non_thresholded_indices)\n",
      "        if n_nonzero_indices < m:\n",
      "            collisions = np.where((np.abs(y)==thresh))[0]\n",
      "            passed = np.random.choice(collisions,m-n_nonzero_indices)\n",
      "            non_thresholded_indices[passed] = 1\n",
      "\n",
      "        y_new = non_thresholded_indices * y\n",
      "\n",
      "        return y_new, thresh\n",
      "\n",
      "    def AIHT_sparse(self, y, X, beta, k, feature_avg=None, alpha=0, example_weights=None, max_iters=10000, tol=10**-16):\n",
      "        \"\"\"Solves DORE accelerated IHT with a sparse matrix X.\n",
      "        \"\"\"\n",
      "        m, n = X.shape\n",
      "        y = np.squeeze(np.asarray(y))\n",
      "        err_norm_prev = 0\n",
      "        beta_0 = beta\n",
      "        beta_prev = beta\n",
      "        X_beta = 0\n",
      "        X_beta_prev = 0\n",
      "        X_beta_twice_prev = 0\n",
      "\n",
      "        if feature_avg is None:\n",
      "            feature_avg = np.zeros(n)\n",
      "    \n",
      "        if example_weights is None:\n",
      "            example_weights = np.ones(m)\n",
      "    \n",
      "        for iter_ in xrange(max_iters):\n",
      "            X_beta_twice_prev = X_beta_prev\n",
      "            X_beta_prev = X_beta\n",
      "            X_beta = (X.dot(beta) - feature_avg.dot(beta))\n",
      "            X_beta = np.squeeze(np.asarray(X_beta))\n",
      "            err = y - example_weights*X_beta\n",
      "            err_reg = -alpha*beta\n",
      "            norm_change = ((np.linalg.norm(beta - beta_prev)**2)/n)\n",
      "            print err.dot(err) + err_reg.dot(err_reg), norm_change, np.linalg.norm(beta)\n",
      "\n",
      "            if iter_ > 0 and (norm_change <= tol):\n",
      "                break\n",
      "\n",
      "            beta_t = beta + np.squeeze(np.asarray(X.T.dot(err))) - err.sum()*feature_avg + alpha*err_reg\n",
      "            beta_t = np.squeeze(np.asarray(beta_t))\n",
      "    \n",
      "            beta_t, thresh = self.thresholder(beta_t,k)\n",
      "            X_beta = X.dot(beta_t) - feature_avg.dot(beta_t)\n",
      "            X_beta = np.squeeze(X_beta)\n",
      "            err = y - example_weights*X_beta\n",
      "            err_reg = -alpha*beta_t\n",
      "    \n",
      "            beta_t_star = beta_t\n",
      "            if iter_ > 2:\n",
      "                delta_X_beta = X_beta - X_beta_prev\n",
      "                delta_regularization = alpha*(beta_t - beta)\n",
      "                dp = delta_X_beta.dot(example_weights*delta_X_beta) + delta_regularization.dot(delta_regularization)\n",
      "                if dp > 0:\n",
      "                    a1 = (delta_X_beta.dot(err) + delta_regularization.dot(err_reg))/dp\n",
      "                    X_beta_1 = (1+a1)*X_beta - a1*X_beta_prev\n",
      "                    beta_1 = beta_t + a1*(beta_t - beta)\n",
      "                    err_1 = y - example_weights*X_beta_1\n",
      "                    err_1_reg = -alpha*beta_1\n",
      "    \n",
      "                    delta_X_beta = X_beta_1 - X_beta_twice_prev\n",
      "                    delta_regularization = alpha*(beta_1 - beta_prev)\n",
      "                    dp = delta_X_beta.dot(example_weights*delta_X_beta) + delta_regularization.dot(delta_regularization)\n",
      "                    if dp > 0:\n",
      "                        a2 = (delta_X_beta.dot(err_1) + delta_regularization.dot(err_1_reg))/dp\n",
      "                        beta_2 = beta_1 + a2*(beta_1 - beta_prev)\n",
      "                        beta_2, thresh = self.thresholder(beta_2,k)\n",
      "    \n",
      "                        X_beta_2 = X.dot(beta_2) - feature_avg.dot(beta_2)\n",
      "                        X_beta_2 = np.squeeze(np.asarray(X_beta_2))\n",
      "                        err_2 = y - example_weights*X_beta_2\n",
      "                        err_reg_2 = -alpha*beta_2\n",
      "    \n",
      "                        if (err_2.dot(err_2) + err_reg_2.dot(err_reg_2)) / (err.dot(err) + err_reg.dot(err_reg)) < 1:\n",
      "                            beta_t_star = beta_2\n",
      "                            X_beta = X_beta_2\n",
      "    \n",
      "            beta_prev = beta\n",
      "            beta = beta_t_star\n",
      "    \n",
      "        return beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = IHTClassifier()\n",
      "sparse_matrix = scipy.sparse.csr_matrix(A)\n",
      "classifier.train(sparse_matrix, b, verbose=True)\n",
      "IHT_weights = classifier.beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Preconditioning matrix\n",
        "Matching pursuits\n",
        "Running iterative hard thresholding"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1027.75074029 0.0 38.389879259\n",
        "1027.71548605 5.33712051793e-05 38.3913656781\n",
        "1027.69562125 2.97975216061e-05 38.3929841186\n",
        "1027.68400317 1.73101385499e-05 38.3946243897\n",
        "1027.66617983 0.000211866277847 38.4025416874\n",
        "1027.66536568 8.0665143432e-06 38.4053093508\n",
        "1027.66529822 5.98313055328e-07 38.4058029388\n",
        "1027.66528899 1.12355843774e-07 38.4051220736\n",
        "1027.66528832 7.05659684553e-09 38.4057081924\n",
        "1027.66528825 6.86634129129e-10 38.405750893\n",
        "1027.66528825 5.08912667552e-11 38.4057269161\n",
        "1027.66528825 3.49655125411e-12 38.4057240693\n",
        "1027.66528825 1.82513822936e-13 38.4057217841\n",
        "1027.66528825 5.87917193313e-15 38.4057218906\n",
        "1027.66528825 4.07763598066e-17 38.4057218987\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################################################################\n",
      "# Sorts words by their weights of the above computed portion                       #\n",
      "####################################################################################\n",
      "\n",
      "print(\"IHT Keywords\")\n",
      "all_terms = [(x, IHT_weights[i]) for i, x in enumerate(features)]\n",
      "all_terms.sort(key=lambda x: x[1], reverse=True)\n",
      "print([x[0] for x in all_terms[:20]])\n",
      "\n",
      "print(\"Lasso Keywords\")\n",
      "for weights in lasso_weights:\n",
      "    all_terms = [(x, weights[i]) for i, x in enumerate(features)]\n",
      "    all_terms.sort(key=lambda x: x[1], reverse=True)\n",
      "    print([x[0] for x in all_terms[:20]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "IHT Keywords\n",
        "[u'actress', u'max', u'scream', u'humor', u'love', u'oscar', u'probably', u'emotional', u'romance', u'friend', u'jack', u'role', u'note', u'set', u'fun', u'sexual', u'wild', u'apparently', u'cool', u'taking']\n",
        "Lasso Keywords\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'success', u'note', u'main', u'humor', u'motion', u'supporting', u'guess', u'person', u'emotional', u'fun', u'example', u'care', u'oscar']\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'success', u'note', u'main', u'humor', u'motion', u'supporting', u'guess', u'person', u'emotional', u'fun', u'example', u'care', u'oscar']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'success', u'note', u'humor', u'main', u'supporting', u'motion', u'guess', u'person', u'emotional', u'fun', u'example', u'care', u'oscar']\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'success', u'note', u'humor', u'main', u'supporting', u'motion', u'guess', u'person', u'emotional', u'fun', u'example', u'care', u'oscar']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'success', u'note', u'humor', u'main', u'supporting', u'motion', u'guess', u'emotional', u'person', u'fun', u'example', u'care', u'oscar']\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'note', u'success', u'humor', u'main', u'supporting', u'motion', u'guess', u'emotional', u'person', u'fun', u'example', u'care', u'oscar']\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'note', u'humor', u'success', u'main', u'supporting', u'motion', u'guess', u'emotional', u'person', u'fun', u'example', u'care', u'oscar']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'note', u'humor', u'success', u'main', u'supporting', u'motion', u'guess', u'emotional', u'person', u'fun', u'example', u'care', u'oscar']\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'humor', u'note', u'success', u'supporting', u'main', u'guess', u'motion', u'emotional', u'person', u'fun', u'example', u'care', u'oscar']\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'humor', u'note', u'supporting', u'success', u'main', u'guess', u'emotional', u'motion', u'person', u'fun', u'example', u'care', u'oscar']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'humor', u'note', u'supporting', u'success', u'main', u'guess', u'emotional', u'motion', u'fun', u'person', u'example', u'care', u'oscar']\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'humor', u'note', u'supporting', u'main', u'success', u'emotional', u'guess', u'motion', u'fun', u'example', u'person', u'care', u'oscar']\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'humor', u'note', u'supporting', u'main', u'success', u'emotional', u'guess', u'motion', u'fun', u'example', u'care', u'person', u'oscar']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'humor', u'note', u'supporting', u'main', u'emotional', u'success', u'guess', u'motion', u'example', u'fun', u'care', u'person', u'oscar']\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'probably', u'instead', u'taking', u'humor', u'note', u'supporting', u'main', u'emotional', u'guess', u'success', u'motion', u'example', u'care', u'fun', u'person', u'oscar']\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'instead', u'probably', u'taking', u'note', u'humor', u'supporting', u'main', u'emotional', u'guess', u'success', u'example', u'motion', u'care', u'fun', u'oscar', u'person']\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'instead', u'probably', u'taking', u'humor', u'note', u'supporting', u'emotional', u'main', u'guess', u'example', u'care', u'success', u'motion', u'fun', u'oscar', u'person']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'poor', u'aspect', u'rated', u'instead', u'probably', u'humor', u'note', u'taking', u'supporting', u'emotional', u'main', u'guess', u'example', u'care', u'success', u'oscar', u'fun', u'motion', u'person']\n",
        "[u'actress', u'poor', u'aspect', u'instead', u'probably', u'rated', u'note', u'humor', u'taking', u'supporting', u'emotional', u'guess', u'main', u'example', u'care', u'oscar', u'fun', u'success', u'motion', u'act']\n",
        "[u'actress', u'poor', u'aspect', u'instead', u'probably', u'rated', u'note', u'humor', u'taking', u'supporting', u'emotional', u'guess', u'main', u'example', u'care', u'oscar', u'fun', u'motion', u'success', u'act']\n",
        "[u'actress', u'poor', u'instead', u'probably', u'aspect', u'rated', u'note', u'humor', u'taking', u'supporting', u'emotional', u'guess', u'example', u'main', u'care', u'oscar', u'fun', u'motion', u'act', u'success']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'poor', u'instead', u'probably', u'aspect', u'note', u'humor', u'rated', u'supporting', u'taking', u'emotional', u'example', u'guess', u'care', u'oscar', u'main', u'fun', u'act', u'motion', u'learn']\n",
        "[u'actress', u'poor', u'instead', u'probably', u'humor', u'note', u'aspect', u'rated', u'supporting', u'emotional', u'example', u'oscar', u'care', u'guess', u'taking', u'main', u'fun', u'act', u'motion', u'learn']\n",
        "[u'actress', u'poor', u'probably', u'instead', u'humor', u'note', u'aspect', u'rated', u'oscar', u'supporting', u'care', u'example', u'emotional', u'guess', u'taking', u'main', u'fun', u'act', u'learn', u'motion']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'poor', u'probably', u'instead', u'humor', u'note', u'oscar', u'rated', u'care', u'aspect', u'supporting', u'example', u'guess', u'emotional', u'taking', u'main', u'fun', u'act', u'learn', u'motion']\n",
        "[u'actress', u'poor', u'humor', u'probably', u'note', u'instead', u'oscar', u'care', u'supporting', u'example', u'guess', u'rated', u'emotional', u'aspect', u'main', u'fun', u'taking', u'act', u'learn', u'motion']\n",
        "[u'actress', u'poor', u'humor', u'note', u'probably', u'instead', u'oscar', u'care', u'example', u'supporting', u'guess', u'emotional', u'rated', u'fun', u'main', u'aspect', u'taking', u'act', u'learn', u'motion']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'poor', u'humor', u'oscar', u'note', u'probably', u'instead', u'care', u'example', u'supporting', u'guess', u'emotional', u'fun', u'rated', u'main', u'aspect', u'act', u'taking', u'learn', u'apparently']\n",
        "[u'actress', u'poor', u'humor', u'oscar', u'note', u'probably', u'instead', u'care', u'example', u'supporting', u'guess', u'fun', u'emotional', u'rated', u'main', u'aspect', u'act', u'taking', u'apparently', u'learn']\n",
        "[u'actress', u'poor', u'humor', u'oscar', u'note', u'probably', u'instead', u'care', u'supporting', u'example', u'guess', u'fun', u'rated', u'emotional', u'act', u'main', u'aspect', u'taking', u'apparently', u'learn']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'poor', u'humor', u'oscar', u'note', u'probably', u'instead', u'care', u'supporting', u'example', u'guess', u'emotional', u'act', u'rated', u'fun', u'main', u'aspect', u'taking', u'apparently', u'learn']\n",
        "[u'actress', u'poor', u'humor', u'oscar', u'note', u'probably', u'supporting', u'care', u'example', u'instead', u'guess', u'rated', u'aspect', u'main', u'act', u'emotional', u'fun', u'taking', u'apparently', u'learn']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'poor', u'humor', u'oscar', u'note', u'supporting', u'care', u'probably', u'example', u'guess', u'instead', u'rated', u'aspect', u'act', u'main', u'emotional', u'apparently', u'fun', u'taking', u'learn']\n",
        "[u'actress', u'poor', u'oscar', u'humor', u'note', u'care', u'supporting', u'example', u'guess', u'probably', u'aspect', u'rated', u'instead', u'act', u'main', u'apparently', u'emotional', u'fun', u'taking', u'learn']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'poor', u'oscar', u'note', u'humor', u'care', u'supporting', u'example', u'guess', u'probably', u'aspect', u'rated', u'act', u'apparently', u'main', u'instead', u'emotional', u'taking', u'fun', u'learn']\n",
        "[u'actress', u'poor', u'oscar', u'note', u'humor', u'supporting', u'care', u'example', u'guess', u'probably', u'aspect', u'rated', u'act', u'apparently', u'main', u'instead', u'emotional', u'taking', u'learn', u'fun']\n",
        "[u'actress', u'oscar', u'poor', u'note', u'humor', u'supporting', u'care', u'example', u'guess', u'aspect', u'apparently', u'rated', u'probably', u'act', u'main', u'learn', u'instead', u'taking', u'emotional', u'fun']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'poor', u'note', u'humor', u'supporting', u'care', u'example', u'guess', u'apparently', u'aspect', u'rated', u'act', u'probably', u'main', u'learn', u'taking', u'basically', u'emotional', u'motion']\n",
        "[u'actress', u'oscar', u'note', u'poor', u'humor', u'supporting', u'care', u'example', u'guess', u'apparently', u'aspect', u'rated', u'act', u'probably', u'main', u'learn', u'basically', u'taking', u'emotional', u'motion']\n",
        "[u'actress', u'oscar', u'note', u'poor', u'supporting', u'care', u'humor', u'guess', u'example', u'apparently', u'aspect', u'rated', u'act', u'learn', u'main', u'probably', u'basically', u'taking', u'motion', u'emotional']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'poor', u'supporting', u'care', u'humor', u'guess', u'apparently', u'example', u'aspect', u'rated', u'act', u'learn', u'basically', u'main', u'probably', u'taking', u'motion', u'lead']\n",
        "[u'actress', u'oscar', u'note', u'poor', u'care', u'supporting', u'humor', u'guess', u'apparently', u'example', u'aspect', u'rated', u'learn', u'act', u'basically', u'main', u'probably', u'taking', u'lead', u'motion']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'poor', u'care', u'supporting', u'guess', u'apparently', u'humor', u'example', u'aspect', u'learn', u'basically', u'rated', u'act', u'main', u'enjoy', u'lead', u'motion', u'probably']\n",
        "[u'actress', u'note', u'oscar', u'poor', u'care', u'supporting', u'guess', u'apparently', u'humor', u'example', u'aspect', u'learn', u'basically', u'rated', u'act', u'enjoy', u'main', u'lead', u'entertainment', u'motion']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'note', u'oscar', u'poor', u'care', u'supporting', u'guess', u'apparently', u'example', u'humor', u'aspect', u'basically', u'learn', u'rated', u'act', u'enjoy', u'lead', u'entertainment', u'main', u'motion']\n",
        "[u'actress', u'note', u'oscar', u'care', u'poor', u'supporting', u'guess', u'apparently', u'example', u'humor', u'basically', u'aspect', u'learn', u'enjoy', u'act', u'entertainment', u'rated', u'lead', u'living', u'main']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'note', u'oscar', u'care', u'poor', u'supporting', u'guess', u'apparently', u'example', u'basically', u'aspect', u'humor', u'learn', u'entertainment', u'enjoy', u'lead', u'act', u'rated', u'living', u'expect']\n",
        "[u'actress', u'note', u'oscar', u'care', u'supporting', u'guess', u'poor', u'apparently', u'basically', u'entertainment', u'example', u'aspect', u'enjoy', u'lead', u'humor', u'learn', u'living', u'act', u'rated', u'expect']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'note', u'oscar', u'care', u'guess', u'supporting', u'poor', u'apparently', u'basically', u'entertainment', u'example', u'enjoy', u'aspect', u'learn', u'lead', u'humor', u'living', u'act', u'expect', u'rated']\n",
        "[u'actress', u'note', u'oscar', u'care', u'guess', u'apparently', u'supporting', u'poor', u'basically', u'entertainment', u'enjoy', u'example', u'lead', u'aspect', u'learn', u'living', u'humor', u'expect', u'act', u'extremely']\n",
        "[u'actress', u'note', u'oscar', u'care', u'guess', u'apparently', u'supporting', u'poor', u'basically', u'entertainment', u'enjoy', u'example', u'lead', u'aspect', u'learn', u'living', u'humor', u'extremely', u'expect', u'act']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'note', u'oscar', u'care', u'guess', u'apparently', u'supporting', u'poor', u'basically', u'entertainment', u'enjoy', u'example', u'lead', u'living', u'aspect', u'learn', u'extremely', u'visit', u'humor', u'expect']\n",
        "[u'actress', u'oscar', u'note', u'care', u'guess', u'apparently', u'supporting', u'basically', u'poor', u'entertainment', u'enjoy', u'lead', u'living', u'example', u'aspect', u'learn', u'extremely', u'visit', u'cool', u'expect']\n",
        "[u'actress', u'oscar', u'note', u'care', u'guess', u'apparently', u'entertainment', u'supporting', u'basically', u'poor', u'enjoy', u'lead', u'living', u'example', u'aspect', u'extremely', u'learn', u'cool', u'visit', u'bruce']\n",
        "[u'actress', u'oscar', u'note', u'guess', u'care', u'apparently', u'entertainment', u'enjoy', u'basically', u'poor', u'supporting', u'living', u'lead', u'extremely', u'aspect', u'example', u'cool', u'visit', u'learn', u'bruce']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'guess', u'care', u'apparently', u'enjoy', u'entertainment', u'basically', u'poor', u'supporting', u'living', u'extremely', u'lead', u'aspect', u'example', u'cool', u'visit', u'score', u'learn']\n",
        "[u'actress', u'oscar', u'note', u'guess', u'apparently', u'care', u'enjoy', u'entertainment', u'basically', u'poor', u'supporting', u'living', u'extremely', u'lead', u'aspect', u'example', u'cool', u'visit', u'score', u'bruce']\n",
        "[u'actress', u'oscar', u'note', u'guess', u'apparently', u'care', u'enjoy', u'entertainment', u'basically', u'poor', u'living', u'extremely', u'supporting', u'lead', u'aspect', u'cool', u'example', u'score', u'visit', u'entertaining']\n",
        "[u'actress', u'oscar', u'note', u'guess', u'apparently', u'enjoy', u'care', u'entertainment', u'basically', u'poor', u'extremely', u'living', u'supporting', u'lead', u'score', u'cool', u'aspect', u'visit', u'example', u'entertaining']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'guess', u'apparently', u'enjoy', u'care', u'extremely', u'basically', u'entertainment', u'poor', u'living', u'lead', u'supporting', u'score', u'aspect', u'cool', u'visit', u'art', u'example']\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'guess', u'apparently', u'care', u'extremely', u'basically', u'entertainment', u'living', u'poor', u'score', u'lead', u'cool', u'supporting', u'art', u'aspect', u'visit', u'entertaining']\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'guess', u'apparently', u'extremely', u'care', u'basically', u'entertainment', u'living', u'poor', u'score', u'lead', u'cool', u'art', u'entertaining', u'supporting', u'visit', u'aspect']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'guess', u'apparently', u'extremely', u'care', u'basically', u'living', u'entertainment', u'poor', u'score', u'cool', u'lead', u'art', u'entertaining', u'visit', u'supporting', u'aspect']\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'extremely', u'guess', u'apparently', u'care', u'basically', u'living', u'entertainment', u'score', u'poor', u'art', u'cool', u'lead', u'entertaining', u'visit', u'aspect', u'surprisingly']\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'extremely', u'guess', u'apparently', u'care', u'basically', u'living', u'score', u'entertainment', u'poor', u'art', u'cool', u'lead', u'entertaining', u'visit', u'surprisingly', u'happen']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'extremely', u'guess', u'apparently', u'living', u'care', u'basically', u'score', u'poor', u'entertainment', u'art', u'cool', u'lead', u'entertaining', u'surprisingly', u'happen', u'visit']\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'extremely', u'guess', u'apparently', u'basically', u'living', u'score', u'care', u'art', u'poor', u'cool', u'entertainment', u'lead', u'entertaining', u'happen', u'surprisingly', u'visit']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'extremely', u'guess', u'apparently', u'living', u'basically', u'score', u'care', u'art', u'cool', u'poor', u'lead', u'entertainment', u'entertaining', u'surprisingly', u'happen', u'aspect']\n",
        "[u'actress', u'oscar', u'note', u'extremely', u'enjoy', u'guess', u'apparently', u'living', u'basically', u'score', u'art', u'care', u'cool', u'poor', u'lead', u'entertaining', u'entertainment', u'surprisingly', u'happen', u'read']\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'extremely', u'guess', u'apparently', u'basically', u'art', u'living', u'score', u'care', u'cool', u'poor', u'lead', u'entertaining', u'entertainment', u'surprisingly', u'happen', u'read']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'extremely', u'enjoy', u'guess', u'apparently', u'art', u'living', u'score', u'basically', u'care', u'cool', u'lead', u'poor', u'entertaining', u'read', u'entertainment', u'surprisingly', u'happen']\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'extremely', u'guess', u'apparently', u'living', u'score', u'art', u'basically', u'cool', u'care', u'lead', u'poor', u'entertaining', u'entertainment', u'read', u'surprisingly', u'happen']\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'extremely', u'guess', u'art', u'score', u'living', u'apparently', u'basically', u'cool', u'care', u'read', u'lead', u'poor', u'entertaining', u'entertainment', u'surprisingly', u'sequel']\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'extremely', u'guess', u'art', u'living', u'score', u'apparently', u'basically', u'cool', u'read', u'care', u'lead', u'poor', u'entertaining', u'surprisingly', u'entertainment', u'sequel']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'extremely', u'enjoy', u'guess', u'art', u'score', u'living', u'apparently', u'basically', u'read', u'cool', u'care', u'lead', u'poor', u'entertaining', u'surprisingly', u'sequel', u'entertainment']\n",
        "[u'actress', u'oscar', u'note', u'extremely', u'enjoy', u'art', u'guess', u'score', u'living', u'read', u'apparently', u'cool', u'basically', u'lead', u'care', u'poor', u'entertaining', u'surprisingly', u'sequel', u'scream']\n",
        "[u'actress', u'oscar', u'note', u'extremely', u'enjoy', u'art', u'guess', u'score', u'living', u'read', u'basically', u'apparently', u'cool', u'lead', u'care', u'poor', u'entertaining', u'sequel', u'surprisingly', u'scream']\n",
        "[u'actress', u'oscar', u'note', u'extremely', u'enjoy', u'art', u'read', u'score', u'guess', u'living', u'cool', u'apparently', u'lead', u'basically', u'care', u'poor', u'scream', u'entertaining', u'surprisingly', u'usually']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'extremely', u'enjoy', u'read', u'art', u'score', u'guess', u'cool', u'living', u'apparently', u'lead', u'basically', u'scream', u'care', u'poor', u'surprisingly', u'entertaining', u'usually']\n",
        "[u'actress', u'oscar', u'note', u'extremely', u'read', u'enjoy', u'score', u'cool', u'art', u'living', u'guess', u'scream', u'apparently', u'lead', u'poor', u'visit', u'basically', u'care', u'usually', u'feature']\n",
        "[u'actress', u'oscar', u'note', u'extremely', u'enjoy', u'read', u'art', u'score', u'living', u'cool', u'guess', u'apparently', u'lead', u'scream', u'basically', u'poor', u'care', u'usually', u'visit', u'surprisingly']\n",
        "[u'actress', u'oscar', u'note', u'extremely', u'enjoy', u'read', u'score', u'art', u'cool', u'living', u'guess', u'scream', u'apparently', u'lead', u'basically', u'poor', u'care', u'usually', u'surprisingly', u'feature']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'read', u'enjoy', u'extremely', u'scream', u'score', u'cool', u'art', u'living', u'lead', u'apparently', u'guess', u'feature', u'york', u'huge', u'basically', u'visit', u'usually']\n",
        "[u'actress', u'oscar', u'note', u'enjoy', u'read', u'extremely', u'living', u'art', u'score', u'cool', u'lead', u'scream', u'apparently', u'guess', u'york', u'dog', u'fiction', u'feature', u'poor', u'visit']\n",
        "[u'actress', u'oscar', u'note', u'extremely', u'enjoy', u'read', u'score', u'art', u'living', u'cool', u'scream', u'guess', u'lead', u'apparently', u'basically', u'feature', u'poor', u'york', u'usually', u'huge']\n",
        "[u'actress', u'oscar', u'note', u'read', u'enjoy', u'extremely', u'living', u'art', u'score', u'cool', u'scream', u'lead', u'apparently', u'guess', u'feature', u'york', u'dog', u'fiction', u'basically', u'visit']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'read', u'enjoy', u'extremely', u'living', u'art', u'score', u'cool', u'lead', u'scream', u'guess', u'apparently', u'york', u'dog', u'feature', u'fiction', u'visit', u'poor']\n",
        "[u'actress', u'oscar', u'note', u'read', u'enjoy', u'extremely', u'living', u'score', u'scream', u'art', u'cool', u'lead', u'york', u'apparently', u'dog', u'feature', u'guess', u'fiction', u'mark', u'visit']\n",
        "[u'actress', u'oscar', u'note', u'extremely', u'read', u'enjoy', u'scream', u'score', u'living', u'cool', u'art', u'apparently', u'lead', u'guess', u'feature', u'huge', u'basically', u'york', u'fiction', u'dog']\n",
        "[u'actress', u'oscar', u'mother', u'read', u'screenplay', u'dog', u'lead', u'living', u'art', u'note', u'york', u'picture', u'romantic', u'fine', u'friend', u'fiction', u'husband', u'feature', u'visit', u'supporting']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'note', u'read', u'extremely', u'enjoy', u'scream', u'living', u'score', u'cool', u'art', u'lead', u'apparently', u'feature', u'york', u'guess', u'dog', u'fiction', u'huge', u'mark']\n",
        "[u'actress', u'oscar', u'read', u'note', u'dog', u'mother', u'living', u'art', u'lead', u'enjoy', u'york', u'score', u'feature', u'fiction', u'fine', u'mark', u'cool', u'visit', u'usually', u'romantic']\n",
        "[u'actress', u'oscar', u'extremely', u'note', u'scream', u'enjoy', u'read', u'score', u'cool', u'living', u'apparently', u'art', u'basically', u'huge', u'guess', u'message', u'lead', u'poor', u'successful', u'feature']\n",
        "[u'actress', u'extremely', u'scream', u'note', u'enjoy', u'oscar', u'score', u'basically', u'read', u'cool', u'apparently', u'message', u'living', u'huge', u'art', u'guess', u'worst', u'sequel', u'coming', u'crew']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'read', u'note', u'enjoy', u'living', u'art', u'extremely', u'score', u'lead', u'scream', u'dog', u'cool', u'fiction', u'feature', u'york', u'apparently', u'usually', u'mark', u'guess']\n",
        "[u'actress', u'extremely', u'oscar', u'note', u'scream', u'score', u'read', u'enjoy', u'basically', u'living', u'apparently', u'message', u'cool', u'art', u'huge', u'surprisingly', u'successful', u'sequel', u'poor', u'annoying']\n",
        "[u'actress', u'oscar', u'read', u'note', u'living', u'extremely', u'enjoy', u'score', u'art', u'scream', u'lead', u'fiction', u'cool', u'dog', u'apparently', u'feature', u'usually', u'mark', u'surprisingly', u'successful']\n",
        "[u'actress', u'oscar', u'read', u'note', u'living', u'extremely', u'enjoy', u'score', u'art', u'scream', u'lead', u'cool', u'fiction', u'dog', u'apparently', u'feature', u'usually', u'mark', u'surprisingly', u'successful']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actress', u'oscar', u'read', u'note', u'living', u'extremely', u'enjoy', u'score', u'art', u'lead', u'scream', u'cool', u'fiction', u'dog', u'apparently', u'feature', u'usually', u'mark', u'surprisingly', u'york']\n",
        "[u'actress', u'oscar', u'note', u'read', u'living', u'fiction', u'score', u'art', u'usually', u'extremely', u'lead', u'surprisingly', u'dog', u'apparently', u'visual', u'successful', u'mark', u'taking', u'difficult', u'success']\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}